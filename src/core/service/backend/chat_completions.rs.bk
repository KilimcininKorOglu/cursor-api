use super::{LastContentType, StreamState};
use crate::{
    app::{
        constant::{
            CHATCMPL_PREFIX, ERR_RESPONSE_RECEIVED, ERR_STREAM_RESPONSE,
            header::{CHUNKED, EVENT_STREAM, JSON, KEEP_ALIVE, NO_CACHE_REVALIDATE},
        },
        lazy::{REAL_USAGE, chat_url},
        model::{
            AppState, Chain, ChainUsage, DateTime, ErrorInfo, ExtToken, LogUpdate, log_manager,
        },
    },
    common::{
        client::{AiServiceRequest, build_client_request},
        model::{error::ChatError, tri::Tri},
        utils::{TrimNewlines as _, get_token_usage, new_uuid_v4},
    },
    core::{
        adapter::openai::*,
        aiserver::v1::EnvironmentInfo,
        config::KeyConfig,
        error::StreamError,
        model::{ExtModel, MessageId, Role, openai::*},
        stream::{
            decoder::{StreamDecoder, StreamMessage, Thinking},
            droppable::DroppableStream,
        },
    },
};
use alloc::{borrow::Cow, sync::Arc};
use atomic_enum::Atomic;
use axum::{Json, body::Body, response::Response};
use bytes::Bytes;
use core::{
    convert::Infallible,
    sync::atomic::{AtomicU32, Ordering},
};
use futures_util::StreamExt as _;
use http::{
    StatusCode,
    header::{CACHE_CONTROL, CONNECTION, CONTENT_LENGTH, CONTENT_TYPE, TRANSFER_ENCODING},
};
use interned::Str;
use std::time::Instant;
use tokio::sync::Mutex;

struct ChatCompletions;

pub struct RunChatCompletionParams {
    pub params: Vec<ChatCompletionMessageParam>,
    pub tools: Vec<ChatCompletionTool>,
    pub environment_info: EnvironmentInfo,
    pub current_config: KeyConfig,
    pub request_time: DateTime,
    pub ext_token: ExtToken,
    pub model: ExtModel,
    pub state: Arc<AppState>,
    pub current_id: u64,
    pub use_pri: bool,
    pub is_stream: bool,
    pub stream_options: ChatCompletionStreamOptions,
}

pub async fn chat_completions(
    params: RunChatCompletionParams,
    usage_check: Option<impl Future<Output = ()> + Send + 'static>,
) -> Result<Response, (StatusCode, Json<OpenAiError>)> {
    let RunChatCompletionParams {
        state,
        params,
        tools,
        ext_token,
        model,
        environment_info,
        current_config,
        current_id,
        use_pri,
        is_stream,
        stream_options,
        request_time,
    } = params;

    let msg_id = uuid::Uuid::new_v4();
    let hex_data = match encode_create_params(
        params,
        tools,
        ext_token.now(),
        model,
        msg_id,
        environment_info,
        current_config.disable_vision,
        current_config.enable_slow_pool,
    )
    .await
    {
        Ok(data) => data,
        Err(e) => {
            log_manager::update_log(current_id, LogUpdate::Failure(e.to_log_error())).await;
            state.decrement_active();
            state.increment_error();
            return Err(e.into_openai_tuple());
        }
    };
    let msg_id = MessageId::new(msg_id.as_bytes());

    let (tx, body) = super::channel_stream(8);
    let _ = tx.send(Ok(hex_data.into())).await;

    // 构建请求客户端
    let req = build_client_request(AiServiceRequest {
        ext_token: &ext_token,
        fs_client_key: None,
        url: chat_url(use_pri),
        stream: true,
        compressed: true,
        trace_id: new_uuid_v4(),
        use_pri,
        cookie: None,
    });
    // 发送请求
    let response = req.body(body).send().await;

    // 处理请求结果
    let response = match response {
        Ok(resp) => {
            // 更新请求日志为成功
            log_manager::update_log(current_id, LogUpdate::Success).await;
            resp
        }
        Err(mut e) => {
            if let Some(url) = e.url_mut() {
                let _ = url.set_host(None);
            }

            // 根据错误类型返回不同的状态码
            let status_code = if e.is_timeout() {
                StatusCode::GATEWAY_TIMEOUT
            } else {
                StatusCode::INTERNAL_SERVER_ERROR
            };
            let e = e.to_string();

            // 更新请求日志为失败
            let error = Str::new(&e);
            log_manager::update_log(current_id, LogUpdate::Failure(ErrorInfo::Simple(error))).await;
            state.decrement_active();
            state.increment_error();

            return Err((status_code, Json(ChatError::RequestFailed(Cow::Owned(e)).to_openai())));
        }
    };

    // 释放活动请求计数
    state.decrement_active();

    let convert_web_ref = current_config.include_web_references;

    if is_stream {
        let message_id = message_id(&msg_id);
        let index = Arc::new(AtomicU32::new(0));
        let start_time = Instant::now();
        let decoder = Arc::new(Mutex::new(StreamDecoder::new()));
        let stream_state = Arc::new(Atomic::new(StreamState::NotStarted));
        let last_content_type = Arc::new(Atomic::new(LastContentType::None));
        let is_need = stream_options.include_usage;

        // 首先处理stream直到获得第一个结果
        let (mut stream, drop_handle) = DroppableStream::new(response.bytes_stream());
        {
            let mut decoder = decoder.lock().await;
            while !decoder.is_first_result_ready() {
                match stream.next().await {
                    Some(Ok(chunk)) => {
                        if let Err(StreamError::Upstream(error)) =
                            decoder.decode(&chunk, convert_web_ref)
                        {
                            let canonical = error.canonical();
                            // 更新请求日志为失败
                            log_manager::update_log(
                                current_id,
                                LogUpdate::Failure2(
                                    canonical.to_error_info(),
                                    start_time.elapsed().as_secs_f64(),
                                ),
                            )
                            .await;
                            state.increment_error();
                            return Err((
                                canonical.status_code(),
                                Json(canonical.into_openai().wrapped()),
                            ));
                        }
                    }
                    Some(Err(e)) => {
                        return Err((
                            StatusCode::INTERNAL_SERVER_ERROR,
                            Json(
                                ChatError::RequestFailed(Cow::Owned(format!(
                                    "Failed to read response chunk: {e}"
                                )))
                                .to_openai(),
                            ),
                        ));
                    }
                    None => {
                        // 更新请求日志为失败
                        log_manager::update_log(
                            current_id,
                            LogUpdate::Failure(ErrorInfo::Simple(Str::from_static(
                                ERR_STREAM_RESPONSE,
                            ))),
                        )
                        .await;
                        state.increment_error();
                        return Err((
                            StatusCode::INTERNAL_SERVER_ERROR,
                            Json(
                                ChatError::RequestFailed(Cow::Borrowed(ERR_STREAM_RESPONSE))
                                    .to_openai(),
                            ),
                        ));
                    }
                }
            }
        }

        let message_id_clone = message_id.clone();
        let decoder_clone = decoder.clone();

        let created = DateTime::utc_now().timestamp();

        // 处理后续的stream
        let stream = stream
            .then(move |chunk| {
                let decoder = decoder_clone.clone();
                let message_id = message_id_clone.clone();
                let index = index.clone();
                let stream_state = stream_state.clone();
                let last_content_type = last_content_type.clone();
                let drop_handle = drop_handle.clone();

                async move {
                    let chunk = match chunk {
                        Ok(c) => c,
                        Err(_) => {
                            // crate::debug_println!("Find chunk error: {e:?}");
                            return Ok::<_, Infallible>(Bytes::new());
                        }
                    };

                    let ctx = MessageProcessContext {
                        message_id: &message_id,
                        model: model.id,
                        index: &index,
                        start_time,
                        current_id,
                        created,
                        stream_state: &stream_state,
                        last_content_type: &last_content_type,
                        is_need,
                    };

                    // 使用decoder处理chunk
                    let messages = match decoder.lock().await.decode(&chunk, convert_web_ref) {
                        Ok(msgs) => msgs,
                        Err(e) => {
                            match e {
                                // 处理普通空流错误
                                StreamError::EmptyStream => {
                                    let empty_stream_count = decoder.lock().await.get_empty_stream_count();
                                    if empty_stream_count > 1 {
                                        eprintln!("[警告] Stream error: empty stream (连续计数: {empty_stream_count})");
                                    }
                                    return Ok(Bytes::new());
                                }
                                // 罕见
                                StreamError::Upstream(e) => {
                                    let message = __unwrap!(serde_json::to_string(&e.canonical().into_openai().wrapped()));
                                    let messages = [StreamMessage::Content(message), StreamMessage::StreamEnd];
                                    return Ok(Bytes::from(process_messages(messages, &ctx).await));
                                }
                            }
                        }
                    };

                    // crate::debug!("{messages:?}");

                    let mut first_response = None;

                    if let Some(first_msg) = decoder.lock().await.take_first_result() {
                        first_response = Some(process_messages(first_msg, &ctx).await);
                    }

                    let current_response = process_messages(messages, &ctx).await;

                    let response_data = if let Some(mut first_response) = first_response {
                        first_response.extend(current_response);
                        first_response
                    } else {
                        current_response
                    };

                    if ctx.stream_state.load(Ordering::Acquire) == StreamState::Completed {
                        drop_handle.drop_stream()
                    }

                    // crate::debug!("{:?}", unsafe{str::from_utf8_unchecked(&response_data)});

                    Ok(Bytes::from(response_data))
                }
            })
            .chain(futures_util::stream::once(async move {
                // 更新delays
                let mut decoder_guard = decoder.lock().await;
                let content_delays = decoder_guard.take_content_delays();
                let thinking_content = decoder_guard.take_thinking_content();

                log_manager::update_log(current_id, LogUpdate::Delays(content_delays, thinking_content))
                    .await;

                let usage = if *REAL_USAGE {
                    let usage =
                        get_token_usage(ext_token, use_pri, request_time, model.id)
                            .await;
                    if let Some(usage) = usage {
                        log_manager::update_log(current_id, LogUpdate::Usage(usage))
                            .await;
                    }
                    usage.map(ChainUsage::into_openai)
                } else {
                    None
                };

                let mut response_data = Vec::with_capacity(128);

                let response = ChatCompletionChunk {
                    id: &message_id,
                    object: (),
                    created: created,
                    model: model.id,
                    choices: Some(chat_completion_chunk::Choice {
                        index: (),
                        delta: Some(chat_completion_chunk::choice::Delta {
                            role: None,
                            content: None,
                            tool_calls: None,
                        }),
                        logprobs: (),
                        finish_reason: Some(if decoder_guard.tool_processed() == 0 {
                            FinishReason::Stop
                        } else {
                            FinishReason::ToolCalls
                        }),
                    }),
                    usage: Tri::Null(is_need),
                };
                extend_from_slice(&mut response_data, &response);

                if is_need {
                    let value = ChatCompletionChunk {
                        id: &message_id,
                        object: (),
                        created,
                        model: model.id,
                        choices: None,
                        usage: Tri::Value(usage.unwrap_or_default()),
                    };
                    extend_from_slice(&mut response_data, &value);
                }

                response_data.extend_from_slice(b"data: [DONE]\n\n");

                if let Some(usage_check) = usage_check {
                    tokio::spawn(usage_check);
                }

                Ok(Bytes::from(response_data))
            }));

        Ok(__unwrap!(
            Response::builder()
                .header(CACHE_CONTROL, NO_CACHE_REVALIDATE)
                .header(CONNECTION, KEEP_ALIVE)
                .header(CONTENT_TYPE, EVENT_STREAM)
                .header(TRANSFER_ENCODING, CHUNKED)
                .body(Body::from_stream(stream))
        ))
    } else {
        // 非流式响应
        let start_time = Instant::now();
        let mut decoder = StreamDecoder::new().no_first_cache();
        let mut thinking_text = String::with_capacity(128);
        let mut full_text = String::with_capacity(128);
        let mut tool_calls = Vec::new();
        let mut stream = response.bytes_stream();
        // let mut prompt = Prompt::None;

        // 逐个处理chunks
        while let Some(chunk) = stream.next().await {
            let chunk = chunk.map_err(|e| {
                (
                    StatusCode::INTERNAL_SERVER_ERROR,
                    Json(
                        ChatError::RequestFailed(Cow::Owned(format!(
                            "Failed to read response chunk: {e}"
                        )))
                        .to_openai(),
                    ),
                )
            })?;

            // 立即处理当前chunk
            match decoder.decode(&chunk, convert_web_ref) {
                Ok(messages) => {
                    for message in messages {
                        match message {
                            StreamMessage::Content(text) => full_text.push_str(&text),
                            StreamMessage::Thinking(Thinking::Text(text)) => {
                                thinking_text.push_str(&text)
                            }
                            StreamMessage::ToolCall(tool_call) => {
                                tool_calls.push(ChatCompletionMessageToolCall::Function {
                                    id: tool_call.id,
                                    function: chat_completion_message_tool_call::Function {
                                        arguments: tool_call.input,
                                        name: tool_call.name,
                                    },
                                })
                            }
                            // StreamMessage::Debug(debug_prompt) => {
                            //     if prompt.is_none() {
                            //         prompt = Prompt::new(debug_prompt);
                            //     } else {
                            //         __cold_path!();
                            //         crate::debug!("UB!2 {debug_prompt:?}");
                            //     }
                            // }
                            _ => {}
                        }
                    }
                }
                Err(StreamError::Upstream(error)) => {
                    let canonical = error.canonical();
                    log_manager::update_log(
                        current_id,
                        LogUpdate::Failure(canonical.to_error_info()),
                    )
                    .await;
                    state.increment_error();
                    return Err((canonical.status_code(), Json(canonical.into_openai().wrapped())));
                }
                Err(StreamError::EmptyStream) => {
                    let empty_stream_count = decoder.get_empty_stream_count();
                    if empty_stream_count > 1 {
                        eprintln!(
                            "[警告] Stream error: empty stream (连续计数: {})",
                            decoder.get_empty_stream_count()
                        );
                    }
                }
            }
        }

        full_text = full_text.trim_leading_newlines();

        // 检查响应是否为空
        if full_text.is_empty() {
            // 更新请求日志为失败
            log_manager::update_log(
                current_id,
                LogUpdate::Failure(ErrorInfo::Simple(Str::from_static(ERR_RESPONSE_RECEIVED))),
            )
            .await;
            state.increment_error();
            return Err((
                StatusCode::INTERNAL_SERVER_ERROR,
                Json(ChatError::RequestFailed(Cow::Borrowed(ERR_RESPONSE_RECEIVED)).to_openai()),
            ));
        }

        let (chain_usage, openai_usage) = if *REAL_USAGE {
            let usage = get_token_usage(ext_token, use_pri, request_time, model.id).await;
            let openai = usage.map(ChainUsage::into_openai);
            (usage, openai)
        } else {
            (None, None)
        };

        let response_data = ChatCompletion {
            id: &{
                let mut buf = [0; 22];
                let mut s = String::with_capacity(31);
                s.push_str(CHATCMPL_PREFIX);
                s.push_str(msg_id.to_str(&mut buf));
                s
            },
            object: (),
            created: DateTime::utc_now().timestamp(),
            model: Some(model.id),
            choices: Some(chat_completion::Choice {
                index: 0,
                finish_reason: if decoder.tool_processed() == 0 {
                    FinishReason::Stop
                } else {
                    FinishReason::ToolCalls
                },
                message: ChatCompletionMessage { role: (), content: Some(full_text), tool_calls },
                logprobs: (),
            }),
            usage: openai_usage,
        };

        // 更新请求日志时间信息和状态
        let total_time = start_time.elapsed().as_secs_f64();
        let content_delays = decoder.take_content_delays();
        let thinking_content = decoder.take_thinking_content();

        log_manager::update_log(
            current_id,
            LogUpdate::TimingChain(
                total_time,
                Chain { delays: content_delays, usage: chain_usage, think: thinking_content },
            ),
        )
        .await;

        if let Some(usage_check) = usage_check {
            tokio::spawn(usage_check);
        }

        let data = __unwrap!(serde_json::to_vec(&response_data));
        Ok(__unwrap!(
            Response::builder()
                .header(CACHE_CONTROL, NO_CACHE_REVALIDATE)
                .header(CONNECTION, KEEP_ALIVE)
                .header(CONTENT_TYPE, JSON)
                .header(CONTENT_LENGTH, data.len())
                .body(Body::from(data))
        ))
    }
}

async fn run() {
    loop {
        
    }
}

// 定义消息处理器的上下文结构体
struct MessageProcessContext<'a> {
    message_id: &'a str,
    model: &'static str,
    index: &'a AtomicU32,
    start_time: Instant,
    stream_state: &'a Atomic<StreamState>,
    last_content_type: &'a Atomic<LastContentType>,
    current_id: u64,
    created: i64,
    is_need: bool,
}

#[inline]
fn extend_from_slice<T>(vector: &mut Vec<u8>, value: &T)
where T: serde::Serialize {
    vector.extend_from_slice(b"data: ");
    let vector = {
        let mut ser = serde_json::Serializer::new(vector);
        __unwrap!(serde::Serialize::serialize(value, &mut ser));
        ser.into_inner()
    };
    vector.extend_from_slice(b"\n\n");
}

// 处理消息并生成响应数据的辅助函数
async fn process_messages<I>(
    messages: impl IntoIterator<Item = I::Item, IntoIter = I>,
    ctx: &MessageProcessContext<'_>,
) -> Vec<u8>
where
    I: Iterator<Item = StreamMessage>,
{
    let mut response_data = Vec::with_capacity(128);

    for message in messages {
        match message {
            StreamMessage::Content(text) => {
                let is_start = ctx.stream_state.load(Ordering::Acquire) == StreamState::NotStarted;
                if is_start {
                    ctx.stream_state.store(StreamState::ContentBlockActive, Ordering::Release);
                }

                let last_type = ctx.last_content_type.load(Ordering::Acquire);
                if last_type != LastContentType::Text {
                    ctx.last_content_type.store(LastContentType::Text, Ordering::Release);
                }

                let chunk = ChatCompletionChunk {
                    id: ctx.message_id,
                    object: (),
                    created: ctx.created,
                    model: ctx.model,
                    choices: Some(chat_completion_chunk::Choice {
                        index: (),
                        delta: Some(chat_completion_chunk::choice::Delta {
                            role: if is_start { Some(Role::Assistant) } else { None },
                            content: Some(Cow::Owned(if is_start {
                                text.trim_leading_newlines()
                            } else {
                                text
                            })),
                            tool_calls: None,
                        }),
                        logprobs: (),
                        finish_reason: None,
                    }),
                    usage: Tri::Null(ctx.is_need),
                };

                extend_from_slice(&mut response_data, &chunk);
            }
            StreamMessage::ToolCall(tool_call) => {
                let is_start = ctx.stream_state.load(Ordering::Acquire) == StreamState::NotStarted;
                if is_start {
                    ctx.stream_state.store(StreamState::ContentBlockActive, Ordering::Release);
                }

                let last_type = ctx.last_content_type.load(Ordering::Acquire);
                if last_type != LastContentType::InputJson {
                    if last_type != LastContentType::None {
                        ctx.index.fetch_add(1, Ordering::AcqRel);
                    }

                    let chunk = ChatCompletionChunk {
                        id: ctx.message_id,
                        object: (),
                        created: ctx.created,
                        model: ctx.model,
                        choices: Some(chat_completion_chunk::Choice {
                            index: (),
                            delta: Some(chat_completion_chunk::choice::Delta {
                                role: if is_start { Some(Role::Assistant) } else { None },
                                content: None,
                                tool_calls: Some(Box::new(chat_completion_chunk::choice::delta::ToolCall {
                                    index: ctx.index.load(Ordering::Acquire),
                                    id: Some(tool_call.id),
                                    function: Some(chat_completion_chunk::choice::delta::tool_call::Function::Start {
                                        name: tool_call.name,
                                        arguments: (),
                                    })
                                })),
                            }),
                            logprobs: (),
                            finish_reason: None,
                        }),
                        usage: Tri::Null(ctx.is_need),
                    };
                    extend_from_slice(&mut response_data, &chunk);

                    ctx.last_content_type.store(LastContentType::InputJson, Ordering::Release);
                }

                let chunk = ChatCompletionChunk {
                    id: ctx.message_id,
                    object: (),
                    created: ctx.created,
                    model: ctx.model,
                    choices: Some(chat_completion_chunk::Choice {
                        index: (),
                        delta: Some(chat_completion_chunk::choice::Delta {
                            role: None,
                            content: None,
                            tool_calls: Some(Box::new(chat_completion_chunk::choice::delta::ToolCall {
                                index: ctx.index.load(Ordering::Acquire),
                                id: None,
                                function: Some(chat_completion_chunk::choice::delta::tool_call::Function::Partial {
                                    arguments: tool_call.input,
                                })
                            })),
                        }),
                        logprobs: (),
                        finish_reason: None,
                    }),
                    usage: Tri::Null(ctx.is_need),
                };
                extend_from_slice(&mut response_data, &chunk);
            }
            StreamMessage::StreamEnd => {
                // 计算总时间和首次片段时间
                let total_time = ctx.start_time.elapsed().as_secs_f64();

                log_manager::update_log(ctx.current_id, LogUpdate::Timing(total_time)).await;

                ctx.stream_state.store(StreamState::Completed, Ordering::Release);
                break;
            }
            _ => {} // 忽略其他消息类型
        }
    }

    response_data
}

fn message_id(id: &MessageId) -> Arc<str> {
    let mut buf = [0; CHATCMPL_PREFIX.len() + 22];

    unsafe {
        core::ptr::copy_nonoverlapping(
            CHATCMPL_PREFIX.as_ptr(),
            buf.as_mut_ptr(),
            CHATCMPL_PREFIX.len(),
        );
        id.to_str(&mut *(buf.as_mut_ptr().add(CHATCMPL_PREFIX.len()) as *mut _ as *mut _));
    }

    let arc = Arc::<[u8]>::from(buf);
    unsafe { Arc::from_raw(Arc::into_raw(arc) as _) }
}
